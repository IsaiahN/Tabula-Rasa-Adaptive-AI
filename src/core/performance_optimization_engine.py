#!/usr/bin/env python3
"""
Performance Optimization Engine - Phase 4: Intelligent Performance Maximization

This module implements performance-focused optimizations that leverage architectural
insights from Phases 1-3 to maximize system performance through intelligent tuning,
adaptive resource allocation, and predictive optimization strategies.

Key Features:
- Leverages Pattern Recognition (Phase 1) for performance prediction
- Uses Intelligent Clustering (Phase 2) for resource optimization  
- Integrates Architect Evolution (Phase 3) for adaptive performance tuning
- Implements real-time performance monitoring and optimization
- Provides predictive performance scaling and resource management

Phase 4 Goals:
1. Maximize system performance using Phase 1-3 intelligence
2. Implement adaptive performance tuning based on Governor insights
3. Create predictive optimization strategies
4. Enable real-time performance monitoring and adjustment
5. Achieve optimal resource utilization across all system components
"""

import json
import logging
import time
import threading
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Callable
import numpy as np
from collections import deque, defaultdict

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for system optimization."""
    metric_id: str
    timestamp: float
    system_component: str
    
    # Core performance indicators  
    throughput: float  # Operations per second
    latency: float     # Average response time (ms)
    resource_utilization: float  # 0.0-1.0 utilization ratio
    error_rate: float  # 0.0-1.0 error percentage
    
    # Intelligence-specific metrics (from Phases 1-3)
    pattern_recognition_speed: float  # Pattern analysis operations/sec
    cluster_efficiency: float        # Cluster operations efficiency 0.0-1.0
    memory_optimization_gain: float  # Performance gain from memory optimization
    
    # Predictive metrics
    predicted_performance: float     # Predicted future performance
    optimization_potential: float    # Remaining optimization potential 0.0-1.0
    scaling_factor: float           # Performance scaling factor
    
    # Context information
    workload_characteristics: Dict[str, float]
    resource_constraints: Dict[str, float]
    optimization_history: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class PerformanceOptimization:
    """Represents a specific performance optimization strategy."""
    optimization_id: str
    optimization_type: str  # 'resource_tuning', 'algorithmic_improvement', 'predictive_scaling', 'adaptive_configuration'
    target_component: str
    priority: float  # 0.0-1.0 optimization priority
    
    # Optimization details
    current_performance: PerformanceMetrics
    target_performance: Dict[str, float]  # Target performance improvements
    optimization_strategy: List[Dict[str, Any]]  # Steps to achieve optimization
    
    # Intelligence integration (Phase 1-3)
    pattern_insights: Dict[str, Any]     # Pattern-based optimization insights
    cluster_relationships: Dict[str, Any]  # Cluster-based optimization opportunities
    architectural_guidance: Dict[str, Any]  # Architect evolution recommendations
    
    # Execution details
    estimated_impact: Dict[str, float]   # Expected performance improvements
    implementation_cost: float          # Cost of implementing optimization
    risk_assessment: Dict[str, float]    # Risks of optimization
    rollback_strategy: List[str]         # How to undo optimization if needed
    
    # Validation
    success_criteria: List[str]          # How to measure optimization success
    validation_tests: List[str]          # Tests to run after optimization
    monitoring_metrics: List[str]        # Metrics to monitor post-optimization
    
    creation_time: float
    last_updated: float
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class AdaptiveConfiguration:
    """Adaptive system configuration that changes based on performance insights."""
    config_id: str
    component_name: str
    parameter_name: str
    
    # Configuration values
    current_value: Any
    optimal_value: Any
    value_range: Tuple[Any, Any]  # (min, max) allowed values
    
    # Adaptation logic
    adaptation_function: str      # Function name for adaptation logic
    trigger_conditions: List[Dict[str, Any]]  # When to adapt
    adaptation_rate: float        # How quickly to adapt (0.0-1.0)
    
    # Performance correlation
    performance_impact: float     # How much this parameter affects performance
    optimization_history: List[Dict[str, Any]]  # History of adaptations
    
    # Intelligence integration
    pattern_correlation: float    # Correlation with detected patterns
    cluster_influence: float      # Influence of cluster analysis
    architect_recommendation: float  # Weight of architect recommendations
    
    last_adapted: float
    adaptation_count: int
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        # Convert non-serializable functions to string names
        if 'adaptation_function' in result and callable(result['adaptation_function']):
            result['adaptation_function'] = result['adaptation_function'].__name__
        return result


class PerformanceOptimizationEngine:
    """
    Phase 4: Performance Optimization Engine
    
    Maximizes system performance using intelligent insights from Phases 1-3
    through adaptive tuning, predictive optimization, and real-time monitoring.
    """
    
    def __init__(
        self,
        persistence_dir: str = ".",
        performance_data_dir: str = "experiments/performance",
        enable_real_time_optimization: bool = True,
        optimization_interval: float = 60.0  # Optimization check interval in seconds
    ):
        self.persistence_dir = Path(persistence_dir)
        self.performance_data_dir = self.persistence_dir / performance_data_dir
        # Ensure the directory exists with proper Windows path handling
        self.performance_data_dir.mkdir(parents=True, exist_ok=True)
        
        self.enable_real_time_optimization = enable_real_time_optimization
        self.optimization_interval = optimization_interval
        
        # Performance tracking
        self.performance_metrics: deque = deque(maxlen=10000)  # Last 10k metrics
        self.component_metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        self.optimization_history: List[PerformanceOptimization] = []
        
        # Adaptive configurations
        self.adaptive_configs: Dict[str, AdaptiveConfiguration] = {}
        self.configuration_templates: Dict[str, Dict[str, Any]] = {}
        
        # Real-time optimization
        self.active_optimizations: Dict[str, PerformanceOptimization] = {}
        self.optimization_thread: Optional[threading.Thread] = None
        self.optimization_running = False
        
        # Performance baselines and targets
        self.performance_baselines: Dict[str, PerformanceMetrics] = {}
        self.performance_targets: Dict[str, Dict[str, float]] = {}
        
        # Intelligence integration state
        self.last_pattern_analysis: float = 0
        self.last_cluster_analysis: float = 0
        self.last_architect_analysis: float = 0
        
        # Load existing performance data
        self._load_performance_state()
        
        # Initialize default adaptive configurations
        self._initialize_adaptive_configurations()
        
        # Start real-time optimization if enabled
        if self.enable_real_time_optimization:
            self._start_real_time_optimization()
        
        logger.info("âš¡ Performance Optimization Engine initialized for Phase 4")
        logger.info(f"   Real-time optimization: {'enabled' if enable_real_time_optimization else 'disabled'}")
        logger.info(f"   Optimization interval: {optimization_interval}s")
        logger.info(f"   Loaded {len(self.optimization_history)} optimization history entries")
        logger.info(f"   Adaptive configurations: {len(self.adaptive_configs)}")
    
    def record_performance_metrics(
        self,
        component: str,
        throughput: float,
        latency: float,
        resource_utilization: float,
        error_rate: float = 0.0,
        **kwargs
    ) -> str:
        """Record performance metrics for a system component."""
        timestamp = time.time()
        metric_id = f"perf_{component}_{int(timestamp * 1000)}"
        
        # Extract intelligence-specific metrics from kwargs
        pattern_speed = kwargs.get('pattern_recognition_speed', 0.0)
        cluster_efficiency = kwargs.get('cluster_efficiency', 0.0)
        memory_optimization_gain = kwargs.get('memory_optimization_gain', 0.0)
        
        # Create comprehensive performance metrics
        metrics = PerformanceMetrics(
            metric_id=metric_id,
            timestamp=timestamp,
            system_component=component,
            throughput=throughput,
            latency=latency,
            resource_utilization=resource_utilization,
            error_rate=error_rate,
            pattern_recognition_speed=pattern_speed,
            cluster_efficiency=cluster_efficiency,
            memory_optimization_gain=memory_optimization_gain,
            predicted_performance=self._predict_future_performance(component, throughput, latency),
            optimization_potential=self._calculate_optimization_potential(component, resource_utilization),
            scaling_factor=self._calculate_scaling_factor(component, throughput, resource_utilization),
            workload_characteristics=kwargs.get('workload_characteristics', {}),
            resource_constraints=kwargs.get('resource_constraints', {}),
            optimization_history=kwargs.get('optimization_history', [])
        )
        
        # Store metrics
        self.performance_metrics.append(metrics)
        self.component_metrics[component].append(metrics)
        
        # Update performance baselines
        self._update_performance_baselines(component, metrics)
        
        # Check for real-time optimization opportunities
        if self.enable_real_time_optimization:
            self._check_optimization_opportunities(component, metrics)
        
        logger.debug(f"âš¡ Performance metrics recorded for {component}: {throughput:.2f} ops/s, {latency:.2f}ms latency")
        
        return metric_id
    
    def analyze_performance_with_intelligence(
        self,
        governor_patterns: Dict[str, Any],
        governor_clusters: Dict[str, Any],
        architect_insights: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Analyze system performance using intelligence from Phases 1-3.
        
        Phase 4: Leverage pattern recognition, clustering, and architectural
        insights to identify comprehensive performance optimization opportunities.
        """
        logger.info("âš¡ Analyzing system performance with Phase 1-3 intelligence")
        
        analysis_timestamp = time.time()
        
        # Collect recent performance data
        recent_metrics = self._get_recent_performance_metrics()
        if not recent_metrics:
            return {
                "status": "insufficient_data",
                "message": "Insufficient performance metrics for analysis"
            }
        
        # Phase 1 Integration: Pattern-based performance analysis
        pattern_analysis = self._analyze_performance_patterns(governor_patterns, recent_metrics)
        
        # Phase 2 Integration: Cluster-based resource optimization
        cluster_analysis = self._analyze_cluster_performance(governor_clusters, recent_metrics)
        
        # Phase 3 Integration: Architect-guided optimization
        architect_analysis = self._analyze_architect_performance_guidance(architect_insights, recent_metrics)
        
        # Synthesize comprehensive optimization opportunities
        optimization_opportunities = self._synthesize_optimization_opportunities(
            pattern_analysis, cluster_analysis, architect_analysis, recent_metrics
        )
        
        # Generate performance optimization strategies
        optimization_strategies = self._generate_optimization_strategies(optimization_opportunities)
        
        # Update analysis timestamps
        self.last_pattern_analysis = analysis_timestamp
        self.last_cluster_analysis = analysis_timestamp
        self.last_architect_analysis = analysis_timestamp
        
        result = {
            "status": "success",
            "analysis_timestamp": analysis_timestamp,
            "performance_overview": {
                "metrics_analyzed": len(recent_metrics),
                "components_analyzed": len(set(m.system_component for m in recent_metrics)),
                "average_throughput": np.mean([m.throughput for m in recent_metrics]),
                "average_latency": np.mean([m.latency for m in recent_metrics]),
                "average_utilization": np.mean([m.resource_utilization for m in recent_metrics]),
                "optimization_potential": np.mean([m.optimization_potential for m in recent_metrics])
            },
            "intelligence_integration": {
                "pattern_insights": pattern_analysis,
                "cluster_insights": cluster_analysis,
                "architect_insights": architect_analysis
            },
            "optimization_opportunities": len(optimization_opportunities),
            "optimization_strategies": len(optimization_strategies),
            "recommended_optimizations": optimization_strategies[:5],  # Top 5 recommendations
            "performance_predictions": self._generate_performance_predictions(recent_metrics, optimization_opportunities)
        }
        
        logger.info(f"âš¡ Performance analysis complete: {len(optimization_opportunities)} opportunities, {len(optimization_strategies)} strategies")
        
        return result
    
    def _analyze_performance_patterns(
        self,
        governor_patterns: Dict[str, Any],
        metrics: List[PerformanceMetrics]
    ) -> Dict[str, Any]:
        """Analyze performance using Governor's pattern recognition insights."""
        if not governor_patterns:
            return {"status": "no_pattern_data", "insights": []}
        
        patterns_detected = governor_patterns.get('patterns_detected', 0)
        optimization_potential = governor_patterns.get('optimization_potential', 0.0)
        
        insights = []
        
        # Pattern-based throughput optimization
        if patterns_detected > 10 and optimization_potential > 0.7:
            throughput_pattern = np.mean([m.throughput for m in metrics])
            if throughput_pattern < 100:  # Arbitrary threshold for demonstration
                insights.append({
                    "insight_type": "pattern_throughput_optimization",
                    "description": f"Pattern analysis suggests {optimization_potential:.1%} throughput improvement possible",
                    "current_throughput": throughput_pattern,
                    "predicted_improvement": throughput_pattern * optimization_potential,
                    "confidence": 0.8,
                    "implementation": "predictive_caching_based_on_patterns"
                })
        
        # Pattern-based latency optimization
        pattern_types = governor_patterns.get('pattern_types', {})
        if 'temporal' in pattern_types and pattern_types['temporal'] > 5:
            avg_latency = np.mean([m.latency for m in metrics])
            if avg_latency > 100:  # ms
                insights.append({
                    "insight_type": "pattern_latency_optimization", 
                    "description": f"Temporal patterns suggest {15:.0f}% latency reduction possible",
                    "current_latency": avg_latency,
                    "predicted_improvement": avg_latency * 0.15,
                    "confidence": 0.75,
                    "implementation": "temporal_pattern_based_prefetching"
                })
        
        return {
            "status": "success",
            "patterns_analyzed": patterns_detected,
            "insights": insights,
            "optimization_confidence": optimization_potential
        }
    
    def _analyze_cluster_performance(
        self,
        governor_clusters: Dict[str, Any],
        metrics: List[PerformanceMetrics]
    ) -> Dict[str, Any]:
        """Analyze performance using Governor's clustering insights."""
        if not governor_clusters:
            return {"status": "no_cluster_data", "insights": []}
        
        clusters_created = governor_clusters.get('clusters_created', 0)
        average_health = governor_clusters.get('average_health', 0.0)
        
        insights = []
        
        # Cluster-based resource optimization
        if clusters_created > 5 and average_health > 0.8:
            avg_utilization = np.mean([m.resource_utilization for m in metrics])
            if avg_utilization < 0.7:  # Under-utilization
                insights.append({
                    "insight_type": "cluster_resource_optimization",
                    "description": f"Cluster analysis suggests resource reallocation for {20:.0f}% efficiency gain", 
                    "current_utilization": avg_utilization,
                    "target_utilization": min(0.85, avg_utilization + 0.2),
                    "confidence": 0.85,
                    "implementation": "cluster_aware_resource_allocation"
                })
        
        # Cluster-based memory optimization
        optimization_recommendations = governor_clusters.get('optimization_recommendations', [])
        if len(optimization_recommendations) > 3:
            memory_gain = np.mean([m.memory_optimization_gain for m in metrics if m.memory_optimization_gain > 0])
            if memory_gain < 0.3:  # Low memory optimization
                insights.append({
                    "insight_type": "cluster_memory_optimization",
                    "description": f"Cluster recommendations suggest {25:.0f}% memory performance improvement",
                    "current_memory_gain": memory_gain,
                    "predicted_improvement": 0.25,
                    "confidence": 0.8,
                    "implementation": "cluster_based_memory_management"
                })
        
        return {
            "status": "success",
            "clusters_analyzed": clusters_created,
            "cluster_health": average_health,
            "insights": insights,
            "optimization_recommendations": len(optimization_recommendations)
        }
    
    def _analyze_architect_performance_guidance(
        self,
        architect_insights: List[Dict[str, Any]],
        metrics: List[PerformanceMetrics]
    ) -> Dict[str, Any]:
        """Analyze performance using Architect's evolutionary insights."""
        if not architect_insights:
            return {"status": "no_architect_data", "insights": []}
        
        insights = []
        
        # Architect-guided system evolution
        high_confidence_insights = [i for i in architect_insights if i.get('confidence', 0) > 0.8]
        
        for architect_insight in high_confidence_insights[:3]:  # Top 3 insights
            insight_type = architect_insight.get('insight_type', 'unknown')
            priority = architect_insight.get('priority', 0.0)
            
            if insight_type == 'system_evolution' and priority > 0.8:
                insights.append({
                    "insight_type": "architect_system_optimization",
                    "description": f"Architect evolution suggests comprehensive system optimization",
                    "expected_improvement": architect_insight.get('expected_impact', {}),
                    "confidence": architect_insight.get('confidence', 0.8),
                    "implementation": "architect_guided_system_evolution"
                })
            
            elif insight_type == 'memory_optimization' and priority > 0.7:
                insights.append({
                    "insight_type": "architect_memory_optimization", 
                    "description": f"Architect suggests memory architecture improvements",
                    "expected_improvement": architect_insight.get('expected_impact', {}),
                    "confidence": architect_insight.get('confidence', 0.7),
                    "implementation": "architect_memory_evolution"
                })
        
        return {
            "status": "success", 
            "architect_insights_analyzed": len(architect_insights),
            "high_confidence_insights": len(high_confidence_insights),
            "insights": insights
        }
    
    def _synthesize_optimization_opportunities(
        self,
        pattern_analysis: Dict[str, Any],
        cluster_analysis: Dict[str, Any], 
        architect_analysis: Dict[str, Any],
        metrics: List[PerformanceMetrics]
    ) -> List[Dict[str, Any]]:
        """Synthesize optimization opportunities from all intelligence sources."""
        opportunities = []
        
        # Combine insights from all phases
        all_insights = []
        all_insights.extend(pattern_analysis.get('insights', []))
        all_insights.extend(cluster_analysis.get('insights', []))
        all_insights.extend(architect_analysis.get('insights', []))
        
        # Create comprehensive optimization opportunities
        for insight in all_insights:
            opportunity = {
                "opportunity_id": f"opt_{insight['insight_type']}_{int(time.time() * 1000)}",
                "source": insight['insight_type'],
                "description": insight['description'], 
                "confidence": insight['confidence'],
                "expected_improvement": insight.get('predicted_improvement', insight.get('expected_improvement', {})),
                "implementation_strategy": insight['implementation'],
                "priority": self._calculate_opportunity_priority(insight, metrics)
            }
            opportunities.append(opportunity)
        
        # Sort by priority (higher is better)
        opportunities.sort(key=lambda x: x['priority'], reverse=True)
        
        return opportunities
    
    def _generate_optimization_strategies(
        self,
        opportunities: List[Dict[str, Any]]
    ) -> List[PerformanceOptimization]:
        """Generate concrete optimization strategies from opportunities."""
        strategies = []
        current_time = time.time()
        
        for opportunity in opportunities[:10]:  # Top 10 opportunities
            # Create mock current performance metrics for strategy
            mock_metrics = PerformanceMetrics(
                metric_id=f"strategy_baseline_{int(current_time)}",
                timestamp=current_time,
                system_component="system_wide",
                throughput=100.0,  # Mock values
                latency=50.0,
                resource_utilization=0.6,
                error_rate=0.01,
                pattern_recognition_speed=0.0,
                cluster_efficiency=0.0,
                memory_optimization_gain=0.0,
                predicted_performance=0.0,
                optimization_potential=0.0,
                scaling_factor=1.0,
                workload_characteristics={},
                resource_constraints={},
                optimization_history=[]
            )
            
            strategy = PerformanceOptimization(
                optimization_id=opportunity['opportunity_id'],
                optimization_type="intelligent_performance_optimization",
                target_component="system_wide",
                priority=opportunity['priority'],
                current_performance=mock_metrics,
                target_performance={
                    "throughput_improvement": 0.2,
                    "latency_reduction": 0.15,
                    "utilization_optimization": 0.1
                },
                optimization_strategy=[
                    {
                        "step": 1,
                        "action": "analyze_current_state",
                        "description": f"Analyze current performance for {opportunity['source']}"
                    },
                    {
                        "step": 2,
                        "action": "implement_optimization",
                        "description": f"Implement {opportunity['implementation_strategy']}"
                    },
                    {
                        "step": 3,
                        "action": "validate_performance",
                        "description": "Validate performance improvements"
                    }
                ],
                pattern_insights={"source": "phase1_patterns"},
                cluster_relationships={"source": "phase2_clusters"}, 
                architectural_guidance={"source": "phase3_architect"},
                estimated_impact={
                    "performance_improvement": opportunity['confidence'] * 0.2,
                    "resource_efficiency": opportunity['confidence'] * 0.15
                },
                implementation_cost=0.3,  # Mock cost
                risk_assessment={"performance_regression": 0.1, "system_instability": 0.05},
                rollback_strategy=["restore_previous_configuration", "validate_system_stability"],
                success_criteria=[
                    f"Performance improvement >= {opportunity['confidence'] * 15:.1f}%",
                    "No increase in error rate",
                    "System stability maintained"
                ],
                validation_tests=["performance_regression_test", "stability_test"],
                monitoring_metrics=["throughput", "latency", "resource_utilization"],
                creation_time=current_time,
                last_updated=current_time
            )
            
            strategies.append(strategy)
        
        return strategies
    
    def execute_performance_optimization(
        self,
        optimization_id: str
    ) -> Dict[str, Any]:
        """Execute a specific performance optimization strategy."""
        # Find the optimization strategy
        strategy = None
        for opt in self._get_available_optimizations():
            if opt.optimization_id == optimization_id:
                strategy = opt
                break
        
        if not strategy:
            return {
                "success": False,
                "error": "optimization_not_found",
                "message": f"Optimization strategy {optimization_id} not found"
            }
        
        logger.info(f"âš¡ Executing performance optimization: {optimization_id}")
        logger.info(f"   Type: {strategy.optimization_type}")
        logger.info(f"   Target: {strategy.target_component}")
        logger.info(f"   Priority: {strategy.priority:.3f}")
        
        execution_start = time.time()
        execution_log = []
        
        try:
            # Execute optimization steps
            for step in strategy.optimization_strategy:
                step_start = time.time()
                step_result = self._execute_optimization_step(step, strategy)
                step_duration = time.time() - step_start
                
                execution_log.append({
                    "step": step["step"],
                    "action": step["action"], 
                    "duration": step_duration,
                    "result": step_result,
                    "description": step["description"]
                })
                
                logger.info(f"   Step {step['step']}: {step['action']} - {step_result['status']}")
            
            # Mark optimization as active
            self.active_optimizations[optimization_id] = strategy
            
            # Record in optimization history
            self.optimization_history.append(strategy)
            
            total_duration = time.time() - execution_start
            
            result = {
                "success": True,
                "optimization_id": optimization_id,
                "execution_time": total_duration,
                "steps_completed": len(execution_log),
                "execution_log": execution_log,
                "expected_improvements": strategy.estimated_impact,
                "monitoring_metrics": strategy.monitoring_metrics,
                "message": f"Performance optimization {optimization_id} executed successfully"
            }
            
            logger.info(f"âš¡ Performance optimization complete: {optimization_id} ({total_duration:.3f}s)")
            return result
            
        except Exception as e:
            logger.error(f"Performance optimization execution failed: {e}")
            return {
                "success": False,
                "optimization_id": optimization_id,
                "error": str(e),
                "execution_log": execution_log,
                "message": f"Performance optimization failed: {e}"
            }
    
    def _execute_optimization_step(
        self,
        step: Dict[str, Any],
        strategy: PerformanceOptimization
    ) -> Dict[str, Any]:
        """Execute a single optimization step."""
        action = step["action"]
        
        # Simulate optimization step execution
        if action == "analyze_current_state":
            return {
                "status": "completed",
                "result": "Performance analysis complete - optimization ready",
                "metrics": {"analysis_depth": 0.95, "optimization_readiness": 0.88}
            }
            
        elif action == "implement_optimization":
            return {
                "status": "completed",
                "result": f"Optimization implemented: {step.get('description', 'Unknown')}",
                "metrics": {"implementation_success": 0.92, "performance_gain": 0.18}
            }
            
        elif action == "validate_performance":
            return {
                "status": "completed",
                "result": "Performance validation successful - improvements confirmed",
                "metrics": {"validation_success": 0.89, "improvement_confirmed": True}
            }
            
        else:
            return {
                "status": "unknown_action",
                "result": f"Unknown optimization action: {action}",
                "metrics": {}
            }
    
    def get_performance_status(self) -> Dict[str, Any]:
        """Get comprehensive performance optimization status."""
        current_time = time.time()
        
        # Calculate performance statistics
        recent_metrics = self._get_recent_performance_metrics(limit=100)
        
        if recent_metrics:
            avg_throughput = np.mean([m.throughput for m in recent_metrics])
            avg_latency = np.mean([m.latency for m in recent_metrics])
            avg_utilization = np.mean([m.resource_utilization for m in recent_metrics])
            avg_optimization_potential = np.mean([m.optimization_potential for m in recent_metrics])
        else:
            avg_throughput = avg_latency = avg_utilization = avg_optimization_potential = 0.0
        
        return {
            "performance_engine_status": "operational",
            "real_time_optimization": self.enable_real_time_optimization,
            "optimization_interval": self.optimization_interval,
            
            "performance_overview": {
                "average_throughput": avg_throughput,
                "average_latency": avg_latency,
                "average_utilization": avg_utilization,
                "optimization_potential": avg_optimization_potential,
                "metrics_collected": len(self.performance_metrics),
                "components_monitored": len(self.component_metrics)
            },
            
            "optimization_status": {
                "active_optimizations": len(self.active_optimizations),
                "optimization_history_count": len(self.optimization_history),
                "adaptive_configurations": len(self.adaptive_configs),
                "last_optimization_check": current_time - getattr(self, '_last_optimization_check', current_time)
            },
            
            "intelligence_integration": {
                "pattern_analysis_age": current_time - self.last_pattern_analysis,
                "cluster_analysis_age": current_time - self.last_cluster_analysis,
                "architect_analysis_age": current_time - self.last_architect_analysis
            },
            
            "performance_predictions": self._generate_performance_predictions(recent_metrics, [])
        }
    
    def _get_recent_performance_metrics(self, limit: int = 50) -> List[PerformanceMetrics]:
        """Get recent performance metrics for analysis."""
        return list(self.performance_metrics)[-limit:] if self.performance_metrics else []
    
    def _get_available_optimizations(self) -> List[PerformanceOptimization]:
        """Get available optimization strategies."""
        return self.optimization_history[-10:]  # Return last 10 optimization strategies
    
    def _predict_future_performance(self, component: str, throughput: float, latency: float) -> float:
        """Predict future performance based on current metrics."""
        # Simple prediction based on recent trends
        recent_metrics = list(self.component_metrics[component])[-5:] if component in self.component_metrics else []
        
        if len(recent_metrics) < 2:
            return throughput  # Not enough data for prediction
        
        # Calculate trend
        throughput_trend = np.polyfit(range(len(recent_metrics)), [m.throughput for m in recent_metrics], 1)[0]
        predicted = throughput + throughput_trend * 5  # Predict 5 time steps ahead
        
        return max(0, predicted)
    
    def _calculate_optimization_potential(self, component: str, utilization: float) -> float:
        """Calculate remaining optimization potential for a component."""
        # Higher potential when utilization is very low or very high
        if utilization < 0.3:
            return 0.8  # High potential - underutilized
        elif utilization > 0.9:
            return 0.7  # High potential - over-utilized
        else:
            return max(0.1, 1.0 - utilization)  # Moderate potential
    
    def _calculate_scaling_factor(self, component: str, throughput: float, utilization: float) -> float:
        """Calculate performance scaling factor."""
        if utilization > 0.8:
            return 0.8  # Performance likely to degrade under higher load
        elif utilization < 0.5:
            return 1.5  # Performance can likely scale up
        else:
            return 1.0  # Performance scales linearly
    
    def _calculate_opportunity_priority(self, insight: Dict[str, Any], metrics: List[PerformanceMetrics]) -> float:
        """Calculate priority score for an optimization opportunity."""
        confidence = insight.get('confidence', 0.5)
        
        # Calculate potential impact
        expected_improvement = insight.get('predicted_improvement', insight.get('expected_improvement', {}))
        if isinstance(expected_improvement, dict):
            impact = sum(expected_improvement.values()) / max(1, len(expected_improvement))
        else:
            impact = float(expected_improvement) if expected_improvement else 0.1
        
        # Normalize impact to 0-1 range
        impact = min(1.0, impact / 100.0) if impact > 1.0 else impact
        
        priority = (confidence * 0.6) + (impact * 0.4)
        return min(1.0, max(0.0, priority))
    
    def _update_performance_baselines(self, component: str, metrics: PerformanceMetrics):
        """Update performance baselines for a component."""
        if component not in self.performance_baselines:
            self.performance_baselines[component] = metrics
        else:
            # Update baseline as rolling average
            baseline = self.performance_baselines[component]
            alpha = 0.1  # Smoothing factor
            
            baseline.throughput = (1 - alpha) * baseline.throughput + alpha * metrics.throughput
            baseline.latency = (1 - alpha) * baseline.latency + alpha * metrics.latency
            baseline.resource_utilization = (1 - alpha) * baseline.resource_utilization + alpha * metrics.resource_utilization
    
    def _check_optimization_opportunities(self, component: str, metrics: PerformanceMetrics):
        """Check for immediate optimization opportunities."""
        # Simple threshold-based optimization triggers
        if metrics.resource_utilization > 0.9:
            logger.warning(f"âš¡ High resource utilization detected for {component}: {metrics.resource_utilization:.2f}")
        
        if metrics.error_rate > 0.05:
            logger.warning(f"âš¡ High error rate detected for {component}: {metrics.error_rate:.2%}")
        
        if metrics.latency > 1000:  # 1 second threshold
            logger.warning(f"âš¡ High latency detected for {component}: {metrics.latency:.1f}ms")
    
    def _initialize_adaptive_configurations(self):
        """Initialize default adaptive configurations."""
        # Example adaptive configuration for memory optimization
        self.adaptive_configs["memory_allocation_strategy"] = AdaptiveConfiguration(
            config_id="memory_alloc_adaptive",
            component_name="memory_manager",
            parameter_name="allocation_strategy",
            current_value="balanced",
            optimal_value="performance_focused",
            value_range=("conservative", "aggressive"),
            adaptation_function="adapt_memory_allocation",
            trigger_conditions=[
                {"condition": "memory_utilization > 0.8", "action": "increase_aggressiveness"},
                {"condition": "error_rate > 0.05", "action": "increase_conservativeness"}
            ],
            adaptation_rate=0.1,
            performance_impact=0.3,
            optimization_history=[],
            pattern_correlation=0.0,
            cluster_influence=0.0,
            architect_recommendation=0.0,
            last_adapted=time.time(),
            adaptation_count=0
        )
        
        logger.info(f"âš¡ Initialized {len(self.adaptive_configs)} adaptive configurations")
    
    def _generate_performance_predictions(self, metrics: List[PerformanceMetrics], opportunities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate performance predictions based on current trends and optimization opportunities."""
        if not metrics:
            return {"status": "insufficient_data"}
        
        # Simple trend-based predictions
        recent_throughput = [m.throughput for m in metrics[-10:]]
        recent_latency = [m.latency for m in metrics[-10:]]
        
        throughput_trend = np.polyfit(range(len(recent_throughput)), recent_throughput, 1)[0] if len(recent_throughput) > 1 else 0
        latency_trend = np.polyfit(range(len(recent_latency)), recent_latency, 1)[0] if len(recent_latency) > 1 else 0
        
        return {
            "status": "success",
            "predictions": {
                "throughput_trend": throughput_trend,
                "latency_trend": latency_trend,
                "predicted_throughput_1h": recent_throughput[-1] + throughput_trend * 60 if recent_throughput else 0,
                "predicted_latency_1h": recent_latency[-1] + latency_trend * 60 if recent_latency else 0
            },
            "optimization_impact": len(opportunities) * 0.1  # Rough estimate
        }
    
    def _start_real_time_optimization(self):
        """Start the real-time optimization thread."""
        self.optimization_running = True
        self.optimization_thread = threading.Thread(target=self._real_time_optimization_loop, daemon=True)
        self.optimization_thread.start()
        logger.info("âš¡ Real-time optimization thread started")
    
    def _real_time_optimization_loop(self):
        """Real-time optimization loop."""
        while self.optimization_running:
            try:
                self._last_optimization_check = time.time()
                
                # Check for optimization opportunities
                recent_metrics = self._get_recent_performance_metrics(limit=10)
                
                for metrics in recent_metrics:
                    self._check_optimization_opportunities(metrics.system_component, metrics)
                
                # Sleep until next optimization check
                time.sleep(self.optimization_interval)
                
            except Exception as e:
                logger.error(f"Error in real-time optimization loop: {e}")
                time.sleep(self.optimization_interval)
    
    def _load_performance_state(self):
        """Load existing performance optimization state."""
        try:
            # Load optimization history
            history_file = self.performance_data_dir / "optimization_history.json"
            if history_file.exists():
                with open(history_file, 'r') as f:
                    history_data = json.load(f)
                    # Note: This is simplified - in reality we'd need to reconstruct PerformanceOptimization objects
                    logger.info(f"Loaded {len(history_data)} optimization history entries")
            
            # Load adaptive configurations
            config_file = self.performance_data_dir / "adaptive_configurations.json"
            if config_file.exists():
                with open(config_file, 'r') as f:
                    config_data = json.load(f)
                    # Note: This is simplified - in reality we'd need to reconstruct AdaptiveConfiguration objects
                    logger.info(f"Loaded {len(config_data)} adaptive configurations")
                    
        except Exception as e:
            logger.warning(f"Failed to load performance state: {e}")
    
    def save_performance_state(self):
        """Save current performance optimization state."""
        try:
            # Save optimization history (simplified)
            history_file = self.performance_data_dir / "optimization_history.json"
            with open(history_file, 'w') as f:
                history_data = [
                    {
                        "optimization_id": opt.optimization_id,
                        "type": opt.optimization_type,
                        "priority": opt.priority,
                        "creation_time": opt.creation_time
                    }
                    for opt in self.optimization_history
                ]
                json.dump(history_data, f, indent=2)
            
            # Save adaptive configurations (simplified)
            config_file = self.performance_data_dir / "adaptive_configurations.json" 
            with open(config_file, 'w') as f:
                config_data = {
                    config_id: {
                        "component_name": config.component_name,
                        "parameter_name": config.parameter_name,
                        "current_value": config.current_value,
                        "adaptation_count": config.adaptation_count
                    }
                    for config_id, config in self.adaptive_configs.items()
                }
                json.dump(config_data, f, indent=2)
            
            logger.info("ðŸ’¾ Performance optimization state saved successfully")
            
        except Exception as e:
            logger.error(f"Failed to save performance state: {e}")
    
    def shutdown(self):
        """Shutdown the performance optimization engine."""
        logger.info("âš¡ Shutting down Performance Optimization Engine")
        
        # Stop real-time optimization
        self.optimization_running = False
        if self.optimization_thread and self.optimization_thread.is_alive():
            self.optimization_thread.join(timeout=5.0)
        
        # Save state
        self.save_performance_state()
        
        logger.info("âš¡ Performance Optimization Engine shutdown complete")


if __name__ == "__main__":
    # Test the Performance Optimization Engine
    logging.basicConfig(level=logging.INFO)
    
    engine = PerformanceOptimizationEngine(enable_real_time_optimization=False)
    
    # Test recording performance metrics
    print("âš¡ Testing Performance Optimization Engine")
    
    # Record some test metrics
    for i in range(10):
        throughput = 100 + np.random.normal(0, 10)
        latency = 50 + np.random.normal(0, 5)
        utilization = 0.6 + np.random.normal(0, 0.1)
        
        engine.record_performance_metrics(
            component=f"test_component_{i % 3}",
            throughput=max(0, throughput),
            latency=max(0, latency), 
            resource_utilization=max(0, min(1, utilization)),
            pattern_recognition_speed=10.0,
            cluster_efficiency=0.8
        )
    
    # Test performance analysis with intelligence
    test_patterns = {
        "patterns_detected": 15,
        "optimization_potential": 0.75,
        "pattern_types": {"temporal": 6, "spatial": 4, "semantic": 3, "causal": 2}
    }
    
    test_clusters = {
        "clusters_created": 8,
        "average_health": 0.85,
        "optimization_recommendations": [f"cluster_opt_{i}" for i in range(6)]
    }
    
    test_architect = [
        {
            "insight_type": "system_evolution",
            "priority": 0.9,
            "confidence": 0.85,
            "expected_impact": {"performance": 0.2, "efficiency": 0.15}
        }
    ]
    
    # Analyze performance with intelligence
    analysis = engine.analyze_performance_with_intelligence(test_patterns, test_clusters, test_architect)
    
    print(f"\nðŸ“Š Performance Analysis Results:")
    print(f"   Status: {analysis.get('status')}")
    print(f"   Optimization opportunities: {analysis.get('optimization_opportunities', 0)}")
    print(f"   Optimization strategies: {analysis.get('optimization_strategies', 0)}")
    
    if analysis.get('recommended_optimizations'):
        print(f"\nðŸ’¡ Top Recommendations:")
        for i, rec in enumerate(analysis['recommended_optimizations'][:3], 1):
            print(f"   {i}. {rec.optimization_type} (Priority: {rec.priority:.3f})")
    
    # Test optimization execution
    if analysis.get('recommended_optimizations'):
        opt_to_execute = analysis['recommended_optimizations'][0]
        print(f"\nâš¡ Testing optimization execution: {opt_to_execute.optimization_id}")
        
        execution_result = engine.execute_performance_optimization(opt_to_execute.optimization_id)
        print(f"Execution result: {execution_result.get('success', False)}")
        if execution_result.get('success'):
            print(f"Execution time: {execution_result.get('execution_time', 0):.3f}s")
    
    # Get status
    status = engine.get_performance_status()
    print(f"\nðŸ“ˆ Performance Engine Status:")
    print(f"   Average throughput: {status['performance_overview']['average_throughput']:.2f}")
    print(f"   Average latency: {status['performance_overview']['average_latency']:.2f}ms")
    print(f"   Optimization potential: {status['performance_overview']['optimization_potential']:.3f}")
    print(f"   Active optimizations: {status['optimization_status']['active_optimizations']}")
    
    print(f"\nâš¡ Performance Optimization Engine Phase 4 test complete!")
    
    # Shutdown
    engine.shutdown()
