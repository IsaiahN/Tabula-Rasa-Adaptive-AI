#!/usr/bin/env python3
"""
Self-Prior Mechanism - Advanced Cognitive Architecture Enhancement

Implements a Self-Prior Module based on Kim et al. research for autonomous
body schema formation and intrinsic goal generation through multimodal sensory
integration and prediction error minimization.

Key Features:
- Multimodal sensory integration (visual, proprioceptive, tactile)
- Density model over sensory experiences for body schema formation
- Active inference integration for intrinsic goal generation
- Autonomous goal-directed behavior based on self-prior alignment
- Integration with existing predictive processing and memory systems

This system enables the agent to develop a "sense of self" through learning
from its own sensory experiences and generating goals that align with its
learned self-model.
"""

import torch
import torch.nn as nn
import numpy as np
import time
import logging
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
from collections import deque
import json
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA

logger = logging.getLogger(__name__)

class SensoryModality(Enum):
    """Types of sensory modalities."""
    VISUAL = "visual"
    PROPRIOCEPTIVE = "proprioceptive"
    TACTILE = "tactile"
    AUDITORY = "auditory"

class GoalType(Enum):
    """Types of intrinsic goals generated by self-prior."""
    EXPLORATION = "exploration"
    NOVELTY_SEEKING = "novelty_seeking"
    SELF_TOUCH = "self_touch"
    REACHING = "reaching"
    PATTERN_COMPLETION = "pattern_completion"
    PREDICTION_ERROR_MINIMIZATION = "prediction_error_minimization"

@dataclass
class SensoryExperience:
    """Represents a multimodal sensory experience."""
    visual_features: np.ndarray
    proprioceptive_state: np.ndarray
    tactile_feedback: Optional[np.ndarray] = None
    auditory_input: Optional[np.ndarray] = None
    timestamp: float = field(default_factory=time.time)
    context: Dict[str, Any] = field(default_factory=dict)
    prediction_error: float = 0.0

@dataclass
class IntrinsicGoal:
    """Represents an intrinsic goal generated by the self-prior mechanism."""
    goal_type: GoalType
    target_state: np.ndarray
    confidence: float
    expected_reward: float
    priority: float
    reasoning: str
    generated_at: float = field(default_factory=time.time)
    completion_threshold: float = 0.8

@dataclass
class SelfPriorState:
    """Current state of the self-prior mechanism."""
    body_schema_confidence: float
    prediction_accuracy: float
    novelty_level: float
    exploration_drive: float
    self_awareness_score: float
    active_goals: List[IntrinsicGoal] = field(default_factory=list)
    recent_experiences: List[SensoryExperience] = field(default_factory=list)

class MultimodalEncoder(nn.Module):
    """Encodes multimodal sensory inputs into a unified representation."""
    
    def __init__(self, 
                 visual_dim: int = 512,
                 proprioceptive_dim: int = 64,
                 tactile_dim: int = 32,
                 auditory_dim: int = 128,
                 latent_dim: int = 256):
        super().__init__()
        
        self.visual_dim = visual_dim
        self.proprioceptive_dim = proprioceptive_dim
        self.tactile_dim = tactile_dim
        self.auditory_dim = auditory_dim
        self.latent_dim = latent_dim
        
        # Individual encoders for each modality
        self.visual_encoder = nn.Sequential(
            nn.Linear(visual_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU()
        )
        
        self.proprioceptive_encoder = nn.Sequential(
            nn.Linear(proprioceptive_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        )
        
        self.tactile_encoder = nn.Sequential(
            nn.Linear(tactile_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU()
        ) if tactile_dim > 0 else None
        
        self.auditory_encoder = nn.Sequential(
            nn.Linear(auditory_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU()
        ) if auditory_dim > 0 else None
        
        # Fusion network
        total_dim = 128 + 32 + (16 if tactile_dim > 0 else 0) + (32 if auditory_dim > 0 else 0)
        self.fusion_network = nn.Sequential(
            nn.Linear(total_dim, 256),
            nn.ReLU(),
            nn.Linear(256, latent_dim),
            nn.Tanh()
        )
        
    def forward(self, visual: torch.Tensor, proprioceptive: torch.Tensor,
                tactile: Optional[torch.Tensor] = None,
                auditory: Optional[torch.Tensor] = None) -> torch.Tensor:
        """Encode multimodal inputs into unified representation."""
        
        # Encode each modality
        visual_encoded = self.visual_encoder(visual)
        proprio_encoded = self.proprioceptive_encoder(proprioceptive)
        
        # Combine encoded features
        features = [visual_encoded, proprio_encoded]
        
        if tactile is not None and self.tactile_encoder is not None:
            tactile_encoded = self.tactile_encoder(tactile)
            features.append(tactile_encoded)
            
        if auditory is not None and self.auditory_encoder is not None:
            auditory_encoded = self.auditory_encoder(auditory)
            features.append(auditory_encoded)
        
        # Fuse all features
        fused = torch.cat(features, dim=-1)
        return self.fusion_network(fused)

class DensityModel:
    """Gaussian Mixture Model for learning self-prior density over sensory experiences."""
    
    def __init__(self, n_components: int = 10, max_samples: int = 10000):
        self.n_components = n_components
        self.max_samples = max_samples
        self.gmm = GaussianMixture(n_components=n_components, random_state=42)
        self.is_fitted = False
        self.samples = deque(maxlen=max_samples)
        
    def add_sample(self, sample: np.ndarray):
        """Add a new sensory experience sample."""
        self.samples.append(sample)
        
    def fit(self):
        """Fit the GMM to accumulated samples."""
        if len(self.samples) < self.n_components * 2:
            return False
            
        X = np.array(list(self.samples))
        self.gmm.fit(X)
        self.is_fitted = True
        return True
        
    def log_likelihood(self, sample: np.ndarray) -> float:
        """Compute log-likelihood of a sample under the learned density."""
        if not self.is_fitted:
            return 0.0
        return self.gmm.score_samples(sample.reshape(1, -1))[0]
        
    def novelty_score(self, sample: np.ndarray) -> float:
        """Compute novelty score (inverse of likelihood)."""
        likelihood = np.exp(self.log_likelihood(sample))
        return 1.0 - likelihood

class IntrinsicGoalGenerator:
    """Generates intrinsic goals based on self-prior and prediction error minimization."""
    
    def __init__(self, 
                 multimodal_encoder: MultimodalEncoder,
                 density_model: DensityModel,
                 goal_types: List[GoalType] = None):
        self.encoder = multimodal_encoder
        self.density_model = density_model
        self.goal_types = goal_types or list(GoalType)
        self.goal_history = deque(maxlen=1000)
        
    def generate_goals(self, 
                      current_experience: SensoryExperience,
                      prediction_error: float,
                      context: Dict[str, Any]) -> List[IntrinsicGoal]:
        """Generate intrinsic goals based on current state and self-prior."""
        
        goals = []
        
        # Encode current experience
        with torch.no_grad():
            visual_tensor = torch.tensor(current_experience.visual_features, dtype=torch.float32)
            proprio_tensor = torch.tensor(current_experience.proprioceptive_state, dtype=torch.float32)
            tactile_tensor = torch.tensor(current_experience.tactile_feedback, dtype=torch.float32) if current_experience.tactile_feedback is not None else None
            auditory_tensor = torch.tensor(current_experience.auditory_input, dtype=torch.float32) if current_experience.auditory_input is not None else None
            
            current_encoding = self.encoder(visual_tensor, proprio_tensor, tactile_tensor, auditory_tensor)
            current_encoding_np = current_encoding.numpy()
        
        # Generate goals based on different criteria
        
        # 1. Novelty-seeking goal
        if GoalType.NOVELTY_SEEKING in self.goal_types:
            novelty = self.density_model.novelty_score(current_encoding_np)
            if novelty > 0.7:  # High novelty threshold
                goals.append(IntrinsicGoal(
                    goal_type=GoalType.NOVELTY_SEEKING,
                    target_state=current_encoding_np,
                    confidence=novelty,
                    expected_reward=novelty * 0.5,
                    priority=novelty,
                    reasoning=f"High novelty detected: {novelty:.3f}"
                ))
        
        # 2. Prediction error minimization goal
        if GoalType.PREDICTION_ERROR_MINIMIZATION in self.goal_types and prediction_error > 0.1:
            goals.append(IntrinsicGoal(
                goal_type=GoalType.PREDICTION_ERROR_MINIMIZATION,
                target_state=current_encoding_np,
                confidence=min(prediction_error, 1.0),
                expected_reward=prediction_error * 0.3,
                priority=prediction_error,
                reasoning=f"Prediction error minimization: {prediction_error:.3f}"
            ))
        
        # 3. Exploration goal (based on low familiarity)
        if GoalType.EXPLORATION in self.goal_types:
            familiarity = 1.0 - self.density_model.novelty_score(current_encoding_np)
            if familiarity < 0.3:  # Low familiarity
                goals.append(IntrinsicGoal(
                    goal_type=GoalType.EXPLORATION,
                    target_state=current_encoding_np,
                    confidence=1.0 - familiarity,
                    expected_reward=(1.0 - familiarity) * 0.4,
                    priority=1.0 - familiarity,
                    reasoning=f"Low familiarity area: {familiarity:.3f}"
                ))
        
        # 4. Pattern completion goal (based on partial patterns)
        if GoalType.PATTERN_COMPLETION in self.goal_types:
            # This would require more sophisticated pattern detection
            # For now, generate based on prediction error patterns
            if prediction_error > 0.05 and prediction_error < 0.3:
                goals.append(IntrinsicGoal(
                    goal_type=GoalType.PATTERN_COMPLETION,
                    target_state=current_encoding_np,
                    confidence=0.6,
                    expected_reward=0.2,
                    priority=0.5,
                    reasoning="Partial pattern detected, attempting completion"
                ))
        
        # Store generated goals
        self.goal_history.extend(goals)
        
        return goals

class SelfPriorManager:
    """
    Main Self-Prior Mechanism manager.
    
    Integrates multimodal sensory experiences, learns body schema through density
    modeling, and generates intrinsic goals for autonomous behavior.
    """
    
    def __init__(self,
                 visual_dim: int = 512,
                 proprioceptive_dim: int = 64,
                 tactile_dim: int = 32,
                 auditory_dim: int = 128,
                 latent_dim: int = 256,
                 n_density_components: int = 10,
                 max_experiences: int = 10000):
        
        # Initialize components
        self.multimodal_encoder = MultimodalEncoder(
            visual_dim, proprioceptive_dim, tactile_dim, auditory_dim, latent_dim
        )
        self.density_model = DensityModel(n_components=n_density_components, max_samples=max_experiences)
        self.goal_generator = IntrinsicGoalGenerator(self.multimodal_encoder, self.density_model)
        
        # State tracking
        self.current_state = SelfPriorState(
            body_schema_confidence=0.0,
            prediction_accuracy=0.0,
            novelty_level=0.0,
            exploration_drive=0.5,
            self_awareness_score=0.0
        )
        
        # Experience tracking
        self.experience_buffer = deque(maxlen=max_experiences)
        self.prediction_errors = deque(maxlen=1000)
        
        # Integration components (will be set by external systems)
        self.predictive_core = None
        self.learning_progress_drive = None
        self.tree_based_director = None
        
        logger.info(f"SelfPriorManager initialized with {n_density_components} density components")
    
    def integrate_components(self, 
                           predictive_core=None,
                           learning_progress_drive=None,
                           tree_based_director=None):
        """Integrate with existing Tabula Rasa components."""
        self.predictive_core = predictive_core
        self.learning_progress_drive = learning_progress_drive
        self.tree_based_director = tree_based_director
        logger.info("SelfPriorManager integrated with existing components")
    
    def process_sensory_experience(self, 
                                 visual_features: np.ndarray,
                                 proprioceptive_state: np.ndarray,
                                 tactile_feedback: Optional[np.ndarray] = None,
                                 auditory_input: Optional[np.ndarray] = None,
                                 context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Process a new multimodal sensory experience."""
        
        context = context or {}
        
        # Create sensory experience
        experience = SensoryExperience(
            visual_features=visual_features,
            proprioceptive_state=proprioceptive_state,
            tactile_feedback=tactile_feedback,
            auditory_input=auditory_input,
            context=context
        )
        
        # Compute prediction error if predictive core is available
        prediction_error = 0.0
        if self.predictive_core:
            try:
                # This would integrate with existing prediction system
                # For now, use a placeholder
                prediction_error = np.random.uniform(0.0, 0.5)  # Placeholder
                experience.prediction_error = prediction_error
            except Exception as e:
                logger.warning(f"Prediction error computation failed: {e}")
        
        # Encode experience
        with torch.no_grad():
            visual_tensor = torch.tensor(visual_features, dtype=torch.float32)
            proprio_tensor = torch.tensor(proprioceptive_state, dtype=torch.float32)
            tactile_tensor = torch.tensor(tactile_feedback, dtype=torch.float32) if tactile_feedback is not None else None
            auditory_tensor = torch.tensor(auditory_input, dtype=torch.float32) if auditory_input is not None else None
            
            encoding = self.multimodal_encoder(visual_tensor, proprio_tensor, tactile_tensor, auditory_tensor)
            encoding_np = encoding.numpy()
        
        # Add to density model
        self.density_model.add_sample(encoding_np)
        
        # Store experience
        self.experience_buffer.append(experience)
        self.prediction_errors.append(prediction_error)
        
        # Update density model periodically
        if len(self.density_model.samples) % 100 == 0:
            self.density_model.fit()
        
        # Generate intrinsic goals
        intrinsic_goals = self.goal_generator.generate_goals(experience, prediction_error, context)
        
        # Update self-prior state
        self._update_self_prior_state(encoding_np, prediction_error, intrinsic_goals)
        
        # Update learning progress drive if available
        if self.learning_progress_drive:
            try:
                # Add self-prior alignment to intrinsic rewards
                novelty = self.density_model.novelty_score(encoding_np)
                self_prior_reward = novelty * 0.1  # Small reward for novelty
                # This would integrate with existing learning progress computation
            except Exception as e:
                logger.warning(f"Learning progress integration failed: {e}")
        
        return {
            'encoding': encoding_np,
            'prediction_error': prediction_error,
            'intrinsic_goals': intrinsic_goals,
            'novelty_score': self.density_model.novelty_score(encoding_np),
            'body_schema_confidence': self.current_state.body_schema_confidence,
            'self_awareness_score': self.current_state.self_awareness_score
        }
    
    def _update_self_prior_state(self, 
                                encoding: np.ndarray, 
                                prediction_error: float,
                                intrinsic_goals: List[IntrinsicGoal]):
        """Update the current self-prior state."""
        
        # Update body schema confidence based on density model fit
        if self.density_model.is_fitted:
            likelihood = np.exp(self.density_model.log_likelihood(encoding))
            self.current_state.body_schema_confidence = likelihood
        
        # Update prediction accuracy
        recent_errors = list(self.prediction_errors)[-100:]  # Last 100 errors
        if recent_errors:
            avg_error = np.mean(recent_errors)
            self.current_state.prediction_accuracy = 1.0 - avg_error
        
        # Update novelty level
        self.current_state.novelty_level = self.density_model.novelty_score(encoding)
        
        # Update exploration drive based on novelty and goals
        exploration_goals = [g for g in intrinsic_goals if g.goal_type in [GoalType.EXPLORATION, GoalType.NOVELTY_SEEKING]]
        if exploration_goals:
            self.current_state.exploration_drive = min(1.0, self.current_state.exploration_drive + 0.1)
        else:
            self.current_state.exploration_drive *= 0.99  # Decay
        
        # Update self-awareness score (composite metric)
        self.current_state.self_awareness_score = (
            self.current_state.body_schema_confidence * 0.3 +
            self.current_state.prediction_accuracy * 0.3 +
            (1.0 - self.current_state.novelty_level) * 0.2 +  # Familiarity
            self.current_state.exploration_drive * 0.2
        )
        
        # Update active goals
        self.current_state.active_goals = intrinsic_goals[:5]  # Keep top 5 goals
        self.current_state.recent_experiences = list(self.experience_buffer)[-10:]  # Last 10 experiences
    
    def get_intrinsic_rewards(self, current_state: Dict[str, Any]) -> Dict[str, float]:
        """Get intrinsic rewards based on self-prior alignment."""
        
        rewards = {}
        
        # Novelty reward
        if 'encoding' in current_state:
            novelty = self.density_model.novelty_score(current_state['encoding'])
            rewards['novelty'] = novelty * 0.1
        
        # Self-prior alignment reward
        if self.current_state.body_schema_confidence > 0.5:
            rewards['self_prior_alignment'] = self.current_state.body_schema_confidence * 0.05
        else:
            rewards['self_prior_alignment'] = 0.0
        
        # Exploration reward
        rewards['exploration'] = self.current_state.exploration_drive * 0.02
        
        # Prediction error minimization reward
        if self.prediction_errors:
            recent_error = self.prediction_errors[-1]
            rewards['prediction_error_minimization'] = (1.0 - recent_error) * 0.03
        
        return rewards
    
    def get_self_prior_metrics(self) -> Dict[str, Any]:
        """Get comprehensive metrics about the self-prior mechanism."""
        
        return {
            'body_schema_confidence': self.current_state.body_schema_confidence,
            'prediction_accuracy': self.current_state.prediction_accuracy,
            'novelty_level': self.current_state.novelty_level,
            'exploration_drive': self.current_state.exploration_drive,
            'self_awareness_score': self.current_state.self_awareness_score,
            'active_goals_count': len(self.current_state.active_goals),
            'total_experiences': len(self.experience_buffer),
            'density_model_fitted': self.density_model.is_fitted,
            'density_components': self.density_model.n_components,
            'recent_prediction_errors': list(self.prediction_errors)[-10:],
            'goal_types_generated': [g.goal_type.value for g in self.current_state.active_goals]
        }

# Factory function for easy integration
def create_self_prior_manager(**kwargs) -> SelfPriorManager:
    """Create a configured self-prior manager."""
    return SelfPriorManager(**kwargs)
