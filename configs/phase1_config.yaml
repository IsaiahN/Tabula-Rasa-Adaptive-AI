# Phase 1 Configuration for Adaptive Learning Agent
# This configuration implements the basic survival environment with
# learning progress drive, memory system, and energy management.

training:
  max_episodes: 100
  max_steps_per_episode: 10000
  checkpoint_interval: 10
  evaluation_interval: 5
  save_dir: "./checkpoints"

agent:
  device: "cpu"  # Change to "cuda" if GPU available
  learning_rate: 0.001

predictive_core:
  visual_size: [3, 64, 64]  # RGB, height, width
  proprioception_size: 11    # Position (3) + orientation (4) + velocity (3) + energy (1)
  hidden_size: 512           # Recurrent hidden state size
  architecture: "lstm"       # "lstm", "gru", or "mamba" (when available)

memory:
  enabled: true
  input_size: 512
  hidden_size: 256
  num_read_heads: 4
  num_write_heads: 1
  memory_size: 512           # Number of memory slots
  word_size: 64              # Size of each memory word

learning_progress:
  smoothing_window: 500      # Steps for error smoothing
  derivative_clamp: [-1.0, 1.0]  # Prevent noise amplification
  boredom_threshold: 0.01    # LP threshold for boredom detection
  boredom_steps: 500         # Steps of low LP before boredom
  lp_weight: 0.7             # Weight for learning progress reward
  empowerment_weight: 0.3    # Weight for empowerment bonus
  use_adaptive_weights: false # Start with fixed weights for stability

energy:
  max_energy: 100.0          # Maximum energy capacity
  base_consumption: 0.01     # Per-step energy consumption
  action_multiplier: 0.5     # Action cost scaling
  computation_multiplier: 0.001  # Computation cost scaling
  food_energy_value: 10.0    # Energy gained from food
  memory_size: 512           # For death manager
  word_size: 64              # For death manager
  use_learned_importance: false  # Use heuristic scoring for Phase 1
  preservation_ratio: 0.2    # Fraction of memories to preserve

goals:
  initial_phase: "survival"  # Start with survival goals only
  environment_bounds: [-10, 10, -10, 10]  # x_min, x_max, y_min, y_max

sleep:
  sleep_trigger_energy: 20.0     # Energy threshold for sleep
  sleep_trigger_boredom_steps: 1000  # Boredom threshold for sleep
  sleep_trigger_memory_pressure: 0.9  # Memory usage threshold for sleep
  sleep_duration_steps: 100      # Steps to spend sleeping
  replay_batch_size: 32          # Batch size for experience replay
  learning_rate: 0.001           # Learning rate during sleep

environment:
  world_size: [20, 20, 5]       # Width, depth, height
  num_food_sources: 5            # Number of food sources
  food_respawn_time: 30.0        # Steps until food respawns
  food_energy_value: 10.0        # Energy value of each food source
  complexity_level: 1            # Starting complexity level
  physics_enabled: true          # Enable basic physics simulation

monitoring:
  log_interval: 100              # Log every N steps
  save_interval: 1000            # Save metrics every N steps
  log_dir: "./logs"              # Directory for log files

# Bootstrap protection parameters
bootstrap:
  protection_steps: 10000        # Steps of reduced energy costs
  energy_cost_reduction: 0.1     # 90% reduction in energy costs
  simplified_environment: true   # Start with simpler environment
  guaranteed_food_access: true   # Ensure food is reachable

# Validation parameters
validation:
  lp_signal_quality_threshold: 2.0    # Minimum signal-to-noise ratio
  stability_score_threshold: 0.8      # Minimum stability score
  memory_utilization_threshold: 0.1   # Minimum memory usage (10%)
  survival_rate_threshold: 0.8        # Minimum survival rate for Phase 2 